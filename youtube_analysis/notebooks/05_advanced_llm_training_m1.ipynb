{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Advanced LLM Training (GPT-2) on M1 Pro with MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Device Configuration for Apple Silicon (MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device found. Using MPS.\")\n",
    "elif torch.cuda.is_available(): # Fallback for environments with CUDA\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device found. Using CUDA.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS or CUDA not available. Using CPU.\")\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cleaned_data_path = '../data/cleaned_watch_history.csv'\n",
    "try:\n",
    "    df = pd.read_csv(cleaned_data_path, parse_dates=['timestamp_utc'])\n",
    "    print(f\"Successfully loaded {cleaned_data_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {cleaned_data_path} was not found. Please ensure 01_data_cleaning.ipynb ran.\")\n",
    "    df = pd.DataFrame(columns=['title', 'video_url', 'channel_name', 'timestamp_utc'])\n",
    "    df['timestamp_utc'] = pd.to_datetime(df['timestamp_utc'])\n",
    "\n",
    "if 'cleaned_title' not in df.columns or df['cleaned_title'].isnull().all():\n",
    "    print(\"'cleaned_title' not found or is empty. Running preprocessing...\")\n",
    "    import nltk\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    try:\n",
    "        stopwords.words('english')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "    try:\n",
    "        word_tokenize('test')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "    punctuations_set = string.punctuation\n",
    "    def preprocess_text_advanced(text):\n",
    "        if pd.isna(text) or not text.strip(): return \"\"\n",
    "        text = str(text).lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in punctuations_set and word.isalpha()]\n",
    "        tokens = [word for word in tokens if word not in stop_words_set]\n",
    "        return ' '.join(tokens)\n",
    "    if 'title' in df.columns:\n",
    "        df['cleaned_title'] = df['title'].apply(preprocess_text_advanced)\n",
    "        print(\"Finished preprocessing 'title' into 'cleaned_title'.\")\n",
    "    else:\n",
    "        print(\"Error: 'title' column missing. 'cleaned_title' will be empty.\")\n",
    "        df['cleaned_title'] = \"\"\n",
    "else:\n",
    "    print(\"'cleaned_title' column found.\")\n",
    "\n",
    "df.dropna(subset=['cleaned_title'], inplace=True)\n",
    "df = df[df['cleaned_title'].str.strip() != '']\n",
    "\n",
    "titles_for_training_gpt2 = df['cleaned_title'].tolist()\n",
    "print(f\"\\nNumber of titles available for GPT-2 training: {len(titles_for_training_gpt2)}\")\n",
    "print(\"Sample titles for training:\")\n",
    "for title in titles_for_training_gpt2[:5]:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Tokenize Text Data for GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_gpt2 = 'gpt2'\n",
    "tokenizer_gpt2 = None\n",
    "encodings_gpt2 = {'input_ids': []}\n",
    "\n",
    "try:\n",
    "    tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name_gpt2)\n",
    "    print(f\"Tokenizer for '{model_name_gpt2}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer for {model_name_gpt2}: {e}\")\n",
    "\n",
    "if tokenizer_gpt2:\n",
    "    if tokenizer_gpt2.pad_token is None:\n",
    "        tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
    "        print(f\"Set tokenizer_gpt2.pad_token to tokenizer_gpt2.eos_token: {tokenizer_gpt2.eos_token}\")\n",
    "\n",
    "    if titles_for_training_gpt2:\n",
    "        encodings_gpt2 = tokenizer_gpt2(titles_for_training_gpt2, truncation=True, padding=True, max_length=128)\n",
    "        print(f\"\\nTokenized {len(encodings_gpt2['input_ids'])} titles for GPT-2.\")\n",
    "        if encodings_gpt2['input_ids']:\n",
    "             print(\"Example of tokenized input_ids for the first title (GPT-2):\")\n",
    "             print(encodings_gpt2['input_ids'][0])\n",
    "    else:\n",
    "        print(\"No titles available for GPT-2 tokenization.\")\n",
    "else:\n",
    "    print(f\"Tokenizer for {model_name_gpt2} not loaded. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_gpt2 = None\n",
    "dataloader_gpt2 = None\n",
    "\n",
    "class YouTubeTitlesDatasetGPT2(Dataset):\n",
    "    def __init__(self, tokenized_input_ids):\n",
    "        self.input_ids = tokenized_input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_input_ids = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
    "        return {'input_ids': item_input_ids, 'labels': item_input_ids.clone()}\n",
    "\n",
    "if encodings_gpt2 and encodings_gpt2['input_ids']:\n",
    "    dataset_gpt2 = YouTubeTitlesDatasetGPT2(encodings_gpt2['input_ids'])\n",
    "    print(f\"\\nCreated GPT-2 dataset with {len(dataset_gpt2)} samples.\")\n",
    "    if len(dataset_gpt2) > 0:\n",
    "      sample_item = dataset_gpt2[0]\n",
    "      print(f\"Sample item from GPT-2 dataset: input_ids shape: {sample_item['input_ids'].shape}\")\n",
    "    \n",
    "    dataloader_gpt2 = DataLoader(dataset_gpt2, batch_size=2, shuffle=True)\n",
    "    print(f\"Created GPT-2 DataLoader with batch_size=2.\")\n",
    "else:\n",
    "    print(\"No tokenized data available for GPT-2 to create Dataset/DataLoader.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Load Pre-trained GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_gpt2 = None\n",
    "if 'model_name_gpt2' in globals() and model_name_gpt2:\n",
    "    try:\n",
    "        model_gpt2 = AutoModelForCausalLM.from_pretrained(model_name_gpt2)\n",
    "        if 'device' in globals(): # Ensure device is defined\n",
    "            model_gpt2.to(device)\n",
    "            print(f\"Pre-trained model '{model_name_gpt2}' loaded successfully and moved to {device}.\")\n",
    "        else:\n",
    "            print(f\"Pre-trained model '{model_name_gpt2}' loaded successfully, but device not defined. Model stays on CPU.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pre-trained model '{model_name_gpt2}': {e}\")\n",
    "else:\n",
    "    print(\"model_name_gpt2 not specified or not found. Cannot load model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Fine-tuning GPT-2 on MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import os\n",
    "\n",
    "gpt2_fine_tuning_done = False\n",
    "# Ensure device, model_gpt2, and dataloader_gpt2 are available\n",
    "if 'device' in globals() and 'model_gpt2' in globals() and model_gpt2 is not None and 'dataloader_gpt2' in globals() and dataloader_gpt2 is not None:\n",
    "    print(f\"Starting fine-tuning of GPT-2 on {device}...\")\n",
    "    optimizer_gpt2 = AdamW(model_gpt2.parameters(), lr=5e-5)\n",
    "    num_epochs_gpt2 = 1 # Start with 1 epoch for testing on M1/MPS\n",
    "\n",
    "    for epoch in range(num_epochs_gpt2):\n",
    "        model_gpt2.train()\n",
    "        total_loss_gpt2 = 0\n",
    "        batch_counter = 0\n",
    "        for batch in dataloader_gpt2:\n",
    "            batch_counter += 1\n",
    "            if batch_counter % 25 == 0: # Print progress every 25 batches\n",
    "                print(f\"  Epoch {epoch+1}, Batch {batch_counter}/{len(dataloader_gpt2)}\")\n",
    "            \n",
    "            optimizer_gpt2.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs_gpt2 = model_gpt2(input_ids, labels=labels)\n",
    "            loss_gpt2 = outputs_gpt2.loss\n",
    "\n",
    "            if loss_gpt2 is not None:\n",
    "                loss_gpt2.backward()\n",
    "                optimizer_gpt2.step()\n",
    "                total_loss_gpt2 += loss_gpt2.item()\n",
    "            else:\n",
    "                print(f\"Warning: Loss is None for batch {batch_counter}. Skipping backward pass.\")\n",
    "        \n",
    "        if len(dataloader_gpt2) > 0:\n",
    "           avg_loss_gpt2 = total_loss_gpt2 / len(dataloader_gpt2)\n",
    "           print(f\"Epoch {epoch+1}/{num_epochs_gpt2} - Average Loss: {avg_loss_gpt2:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs_gpt2} - DataLoader is empty.\")\n",
    "        \n",
    "        # Optional: Synchronize MPS device at the end of epoch if needed for debugging, usually not required for basic loops.\n",
    "        # if device.type == 'mps':\n",
    "        #     torch.mps.synchronize()\n",
    "    print(\"GPT-2 Fine-tuning completed.\")\n",
    "    gpt2_fine_tuning_done = True\n",
    "else:\n",
    "    print(\"Device, model_gpt2, or dataloader_gpt2 not available. Skipping GPT-2 fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Save Fine-tuned GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gpt2 = './fine_tuned_gpt2_youtube_titles'\n",
    "if 'gpt2_fine_tuning_done' in globals() and gpt2_fine_tuning_done and 'model_gpt2' in globals() and model_gpt2 is not None and 'tokenizer_gpt2' in globals() and tokenizer_gpt2 is not None:\n",
    "    try:\n",
    "        os.makedirs(output_dir_gpt2, exist_ok=True)\n",
    "        model_gpt2.save_pretrained(output_dir_gpt2)\n",
    "        tokenizer_gpt2.save_pretrained(output_dir_gpt2)\n",
    "        print(f\"GPT-2 Model and tokenizer saved to {output_dir_gpt2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving GPT-2 model/tokenizer: {e}\")\n",
    "elif not ('gpt2_fine_tuning_done' in globals() and gpt2_fine_tuning_done):\n",
    "    print(\"GPT-2 fine-tuning was not performed or completed. Skipping saving model.\")\n",
    "else:\n",
    "    print(\"GPT-2 Model or Tokenizer not available. Skipping saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Load Fine-tuned GPT-2 for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer # Ensure these are imported\n",
    "import torch # Ensure torch is imported\n",
    "import os\n",
    "\n",
    "# Ensure 'device' is available from the first cell, or redefine if necessary for this cell's context\n",
    "if 'device' not in globals():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Device for inference re-initialized to: {device}\")\n",
    "\n",
    "fine_tuned_output_dir_gpt2 = './fine_tuned_gpt2_youtube_titles' # Should match the saving directory\n",
    "inference_model_gpt2 = None\n",
    "inference_tokenizer_gpt2 = None\n",
    "\n",
    "if os.path.exists(fine_tuned_output_dir_gpt2):\n",
    "    try:\n",
    "        inference_model_gpt2 = AutoModelForCausalLM.from_pretrained(fine_tuned_output_dir_gpt2)\n",
    "        inference_tokenizer_gpt2 = AutoTokenizer.from_pretrained(fine_tuned_output_dir_gpt2)\n",
    "        print(f\"Fine-tuned GPT-2 model and tokenizer loaded successfully from {fine_tuned_output_dir_gpt2}.\")\n",
    "        \n",
    "        inference_model_gpt2.to(device)\n",
    "        inference_model_gpt2.eval()\n",
    "        print(f\"GPT-2 inference model moved to {device} and set to evaluation mode.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned GPT-2 model or tokenizer: {e}\")\n",
    "else:\n",
    "    print(f\"Fine-tuned GPT-2 model directory not found: {fine_tuned_output_dir_gpt2}. Please ensure the model was trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Text Generation with Fine-tuned GPT-2 (on MPS/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_gpt2(prompt, model, tokenizer, device, max_length=50, num_return_sequences=1):\n",
    "    if not model or not tokenizer:\n",
    "        print(\"Inference model or tokenizer not available for GPT-2 generation.\")\n",
    "        return []\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        generated_texts = [tokenizer.decode(seq, skip_special_tokens=True) for seq in output_sequences]\n",
    "        return generated_texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT-2 text generation: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure device is defined in this scope, or passed correctly\n",
    "if 'inference_model_gpt2' in globals() and inference_model_gpt2 and 'inference_tokenizer_gpt2' in globals() and inference_tokenizer_gpt2 and 'device' in globals():\n",
    "    \n",
    "    prompts_gpt2 = [\n",
    "        \"How to build\", \n",
    "        \"The future of\", \n",
    "        \"Exploring the secrets of\",\n",
    "        \"Advanced tutorial on\",\n",
    "        \"Why is python\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n--- GPT-2 Text Generation Demo ---\")\n",
    "    for p in prompts_gpt2:\n",
    "        generated_gpt2 = generate_text_gpt2(p, inference_model_gpt2, inference_tokenizer_gpt2, device, max_length=40)\n",
    "        print(f\"\\nPrompt: {p}...\")\n",
    "        if generated_gpt2:\n",
    "            for i, g in enumerate(generated_gpt2):\n",
    "                print(f\"Generated {i+1}: {g}\")\n",
    "        else:\n",
    "            print(\"  (No text generated or error occurred)\")\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"GPT-2 inference model, tokenizer, or device not loaded/defined. Cannot demonstrate text generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
