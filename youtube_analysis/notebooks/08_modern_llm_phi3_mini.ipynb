{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Phi-3 Mini on Video Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries if not already present\n",
    "# Using -q for quieter output\n",
    "# !pip install -q transformers torch accelerate einops pandas\n",
    "# Ensure you have a version of transformers that supports Phi-3 (e.g., >= 4.39 or newer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "# import einops # Often a dependency for newer models, import if needed by Phi-3 or training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration for Apple Silicon (MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "device_phi3 = accelerator.device\n",
    "print(f\"Selected device: {device_phi3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Video Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_summaries_file = '../data/video_summaries.jsonl'\n",
    "loaded_summaries_list = []\n",
    "try:\n",
    "    with open(input_summaries_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            loaded_summaries_list.append(json.loads(line))\n",
    "    df_phi3_summaries = pd.DataFrame(loaded_summaries_list)\n",
    "    if not df_phi3_summaries.empty:\n",
    "        print(f\"Successfully loaded {len(df_phi3_summaries)} summaries from {input_summaries_file}.\")\n",
    "        df_phi3_summaries.dropna(subset=['summary'], inplace=True)\n",
    "        df_phi3_summaries = df_phi3_summaries[df_phi3_summaries['summary'].str.strip() != '']\n",
    "        print(f\"Number of summaries after filtering empty/NaN: {len(df_phi3_summaries)}\")\n",
    "    else:\n",
    "        print(f\"Loaded summaries file {input_summaries_file}, but it resulted in an empty DataFrame.\")\n",
    "        df_phi3_summaries = pd.DataFrame(columns=['video_id', 'title', 'original_transcript_length', 'summary'])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Summaries file {input_summaries_file} not found. Please run notebook 07 (summarization part) first.\")\n",
    "    df_phi3_summaries = pd.DataFrame(columns=['video_id', 'title', 'original_transcript_length', 'summary'])\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON from {input_summaries_file}: {e}\")\n",
    "    df_phi3_summaries = pd.DataFrame(columns=['video_id', 'title', 'original_transcript_length', 'summary'])\n",
    "\n",
    "if 'df_phi3_summaries' in globals() and df_phi3_summaries.empty:\n",
    "    print(\"\\nWarning: No summaries loaded. This notebook relies on summary data for fine-tuning Phi-3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Phi-3 Mini Tokenizer and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi3_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer_phi3 = None\n",
    "try:\n",
    "    tokenizer_phi3 = AutoTokenizer.from_pretrained(phi3_model_id, trust_remote_code=True)\n",
    "    print(f\"Tokenizer for '{phi3_model_id}' loaded successfully.\")\n",
    "    if tokenizer_phi3.pad_token is None:\n",
    "        tokenizer_phi3.pad_token = tokenizer_phi3.eos_token \n",
    "        print(f\"Set tokenizer_phi3.pad_token to: '{tokenizer_phi3.pad_token}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Phi-3 tokenizer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The 'microsoft/Phi-3-mini-4k-instruct' model is an instruction-tuned model. For fine-tuning on plain text (like our summaries), we are treating each summary as a sequence for the model to learn to complete or generate in a similar style. For more conversational tasks, one would typically format the input using the model's specific chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_phi3_input_ids = []\n",
    "encodings_phi3_attention_mask = []\n",
    "\n",
    "if 'df_phi3_summaries' in globals() and not df_phi3_summaries.empty and 'summary' in df_phi3_summaries.columns and tokenizer_phi3 is not None:\n",
    "    summary_list_phi3 = df_phi3_summaries['summary'].tolist()\n",
    "    if summary_list_phi3:\n",
    "        try:\n",
    "            encodings_phi3 = tokenizer_phi3(\n",
    "                summary_list_phi3, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=512, \n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            encodings_phi3_input_ids = encodings_phi3['input_ids']\n",
    "            encodings_phi3_attention_mask = encodings_phi3['attention_mask']\n",
    "            print(f\"Tokenized {len(encodings_phi3_input_ids)} summaries for Phi-3.\")\n",
    "            if encodings_phi3_input_ids:\n",
    "                print(f\"Example tokenized summary for Phi-3 (first 10 tokens): {encodings_phi3_input_ids[0][:10]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Phi-3 summary tokenization: {e}\")\n",
    "    else:\n",
    "        print(\"Summary list for Phi-3 is empty. No summaries to tokenize.\")\n",
    "else:\n",
    "    print(\"DataFrame of summaries for Phi-3 is empty, 'summary' column is missing, or tokenizer_phi3 not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset and DataLoader for Phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "dataset_phi3_summaries = None\n",
    "dataloader_phi3_summaries = None\n",
    "\n",
    "class Phi3SummaryDataset(Dataset):\n",
    "    def __init__(self, input_ids_list, attention_masks_list):\n",
    "        self.input_ids_list = input_ids_list\n",
    "        self.attention_masks_list = attention_masks_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids_list[idx]\n",
    "        attention_mask = self.attention_masks_list[idx]\n",
    "        if not isinstance(input_ids, list) or not isinstance(attention_mask, list):\n",
    "            print(f\"Warning: Data at index {idx} is not in list format. Skipping.\")\n",
    "            return {'input_ids': torch.tensor([], dtype=torch.long), \n",
    "                    'attention_mask': torch.tensor([], dtype=torch.long),\n",
    "                    'labels': torch.tensor([], dtype=torch.long)}\n",
    "        return {'input_ids': torch.tensor(input_ids, dtype=torch.long), \n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'labels': torch.tensor(input_ids, dtype=torch.long).clone()} \n",
    "\n",
    "if encodings_phi3_input_ids and encodings_phi3_attention_mask: \n",
    "    if len(encodings_phi3_input_ids) == len(encodings_phi3_attention_mask):\n",
    "        dataset_phi3_summaries = Phi3SummaryDataset(encodings_phi3_input_ids, encodings_phi3_attention_mask)\n",
    "        print(f\"Created Phi-3 summary dataset with {len(dataset_phi3_summaries)} samples.\")\n",
    "        if len(dataset_phi3_summaries) > 0:\n",
    "            dataloader_phi3_summaries = DataLoader(dataset_phi3_summaries, batch_size=1, shuffle=True)\n",
    "            print(f\"Created Phi-3 summary DataLoader with batch_size=1.\")\n",
    "        else:\n",
    "            print(\"Phi-3 summary dataset is empty. DataLoader not created.\")\n",
    "    else:\n",
    "        print(\"Mismatch between length of input_ids and attention_mask for Phi-3. Cannot create dataset.\")\n",
    "else:\n",
    "    print(\"No tokenized summaries or attention masks available for Phi-3. Cannot create Dataset/DataLoader.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Phi-3 Mini Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phi3 = None\n",
    "if 'phi3_model_id' in globals() and phi3_model_id and 'device_phi3' in globals():\n",
    "    try:\n",
    "        model_phi3 = AutoModelForCausalLM.from_pretrained(\n",
    "            phi3_model_id, \n",
    "            trust_remote_code=True, \n",
    "            torch_dtype=\"auto\" \n",
    "        )\n",
    "        print(f\"Pre-trained model '{phi3_model_id}' loaded successfully. It will be moved to {device_phi3} by Accelerator.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pre-trained Phi-3 model '{phi3_model_id}': {e}\")\n",
    "else:\n",
    "    print(\"phi3_model_id or device_phi3 not defined. Cannot load model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following attempts full fine-tuning. This is memory-intensive for a ~3.8B parameter model on devices with limited VRAM/unified memory. If this fails, consider PEFT/LoRA (see section below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting Full Fine-tuning of Phi-3 Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "phi3_full_finetune_successful = False\n",
    "\n",
    "if 'model_phi3' in globals() and model_phi3 is not None and \\ \n",
    "   'dataloader_phi3_summaries' in globals() and dataloader_phi3_summaries is not None and \\ \n",
    "   'accelerator' in globals():\n",
    "    \n",
    "    print(f\"Preparing for full fine-tuning of Phi-3 on {accelerator.device}...\")\n",
    "    optimizer_phi3 = AdamW(model_phi3.parameters(), lr=3e-5) \n",
    "\n",
    "    model_phi3, optimizer_phi3, dataloader_phi3_summaries = accelerator.prepare(\n",
    "        model_phi3, optimizer_phi3, dataloader_phi3_summaries\n",
    "    )\n",
    "    \n",
    "    num_epochs_phi3 = 1 \n",
    "\n",
    "    print(\"Starting full fine-tuning of Phi-3 Mini...\")\n",
    "    try:\n",
    "        for epoch in range(num_epochs_phi3):\n",
    "            model_phi3.train()\n",
    "            total_loss_phi3 = 0\n",
    "            print(f\"Starting Phi-3 Fine-Tuning Epoch {epoch+1}/{num_epochs_phi3}\")\n",
    "            for batch_idx, batch in enumerate(dataloader_phi3_summaries):\n",
    "                optimizer_phi3.zero_grad()\n",
    "                \n",
    "                outputs = model_phi3(\n",
    "                    input_ids=batch['input_ids'], \n",
    "                    attention_mask=batch['attention_mask'], \n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                if loss is not None:\n",
    "                    accelerator.backward(loss) \n",
    "                    optimizer_phi3.step()\n",
    "                    total_loss_phi3 += loss.item()\n",
    "                    if batch_idx % 1 == 0: \n",
    "                       print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{len(dataloader_phi3_summaries)}, Loss: {loss.item():.4f}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Loss is None for Phi-3 batch {batch_idx+1}. Skipping.\")\n",
    "            \n",
    "            avg_loss_phi3 = total_loss_phi3 / len(dataloader_phi3_summaries) if len(dataloader_phi3_summaries) > 0 else 0\n",
    "            accelerator.print(f\"Epoch {epoch+1}/{num_epochs_phi3} - Average Training Loss: {avg_loss_phi3:.4f}\")\n",
    "        \n",
    "        phi3_full_finetune_successful = True\n",
    "        print(\"Full fine-tuning of Phi-3 Mini completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during full fine-tuning of Phi-3: {e}\")\n",
    "        print(\"Full fine-tuning may have failed due to memory (OOM) or other issues. Consider PEFT/LoRA as an alternative.\")\n",
    "else:\n",
    "    print(\"Required variables (model_phi3, dataloader_phi3_summaries, accelerator) not available. Skipping full fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Fully Fine-tuned Phi-3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "output_dir_phi3_full = './fine_tuned_phi3_mini_summaries_full'\n",
    "\n",
    "if 'phi3_full_finetune_successful' in globals() and phi3_full_finetune_successful and \\ \n",
    "   'model_phi3' in globals() and model_phi3 is not None and \\ \n",
    "   'tokenizer_phi3' in globals() and tokenizer_phi3 is not None and \\ \n",
    "   'accelerator' in globals():\n",
    "    try:\n",
    "        os.makedirs(output_dir_phi3_full, exist_ok=True)\n",
    "        unwrapped_model_phi3 = accelerator.unwrap_model(model_phi3)\n",
    "        unwrapped_model_phi3.save_pretrained(output_dir_phi3_full)\n",
    "        tokenizer_phi3.save_pretrained(output_dir_phi3_full) \n",
    "        print(f\"Fully fine-tuned Phi-3 model and tokenizer saved to {output_dir_phi3_full}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving fully fine-tuned Phi-3 model/tokenizer: {e}\")\n",
    "elif not ('phi3_full_finetune_successful' in globals() and phi3_full_finetune_successful):\n",
    "    print(\"Phi-3 full fine-tuning was not completed or successful. Model not saved.\")\n",
    "else:\n",
    "    print(\"Phi-3 model, tokenizer, or accelerator not available. Skipping saving of fully fine-tuned model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: PEFT/LoRA Fine-tuning (If Full Fine-tuning Fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If full fine-tuning is too resource-intensive, PEFT methods like LoRA can be used.\n",
    "# This involves freezing most of the base model and training only small adapter layers.\n",
    "# Example outline (requires `peft` library: !pip install -q peft):\n",
    "#\n",
    "# from peft import get_peft_model, LoraConfig, TaskType\n",
    "#\n",
    "# # It's often good to reload the base model to ensure no prior full fine-tuning attempts affect it,\n",
    "# # especially if using a different precision like float16 for LoRA.\n",
    "# # Ensure 'phi3_model_id' and 'device_phi3' are defined.\n",
    "# # model_phi3_for_lora = AutoModelForCausalLM.from_pretrained(\n",
    "# #     phi3_model_id, \n",
    "# #     trust_remote_code=True, \n",
    "# #     torch_dtype=torch.float16 # Example: use float16 for LoRA if supported\n",
    "# # ).to(device_phi3)\n",
    "#\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,  # Rank of the update matrices.\n",
    "#     lora_alpha=32,  # Alpha parameter for LoRA scaling.\n",
    "#     # For Phi-3, target_modules might include: \"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\" \n",
    "#     # or specific names like \"Wqkv\", \"out_proj\", \"fc1\", \"fc2\" depending on the exact architecture variant.\n",
    "#     # It's crucial to inspect model_phi3.named_modules() to identify correct target layers.\n",
    "#     target_modules=[\"Wqkv\", \"out_proj\"], # Placeholder - MUST BE VERIFIED FOR PHI-3\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.CAUSAL_LM\n",
    "# )\n",
    "#\n",
    "# # model_phi3_lora = get_peft_model(model_phi3_for_lora, lora_config)\n",
    "# # model_phi3_lora.print_trainable_parameters()\n",
    "#\n",
    "# # # Prepare for training with Accelerator if using LoRA model\n",
    "# # optimizer_phi3_lora = AdamW(model_phi3_lora.parameters(), lr=1e-4) # May use different LR for LoRA\n",
    "# # model_phi3_lora, optimizer_phi3_lora, dataloader_phi3_lora_prepared = accelerator.prepare(\n",
    "# #     model_phi3_lora, optimizer_phi3_lora, dataloader_phi3_summaries # Reuse the same dataloader\n",
    "# # )\n",
    "#\n",
    "# # # ... then, proceed with a similar training loop using model_phi3_lora and dataloader_phi3_lora_prepared ...\n",
    "#\n",
    "# # # Save LoRA adapters and tokenizer:\n",
    "# # output_dir_phi3_lora = './fine_tuned_phi3_mini_summaries_lora'\n",
    "# # if model_phi3_lora and tokenizer_phi3: # Check if LoRA model and tokenizer exist\n",
    "# #     os.makedirs(output_dir_phi3_lora, exist_ok=True)\n",
    "# #     unwrapped_lora_model = accelerator.unwrap_model(model_phi3_lora) # Unwrap if prepared with accelerator\n",
    "# #     unwrapped_lora_model.save_pretrained(output_dir_phi3_lora)\n",
    "# #     tokenizer_phi3.save_pretrained(output_dir_phi3_lora)\n",
    "# #     print(f\"LoRA adapters and tokenizer saved to {output_dir_phi3_lora}\")\n",
    "# # else:\n",
    "# #     print(\"LoRA model or tokenizer not available. Skipping saving.\")\n",
    "\n",
    "print(\"PEFT/LoRA section provides a conceptual outline. \\n\",\n",
    "      \"Actual implementation requires installing 'peft', identifying correct 'target_modules' for Phi-3, \\n\",\n",
    "      \"and ensuring the training loop is correctly set up for the PEFT model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fine-tuned Phi-3 Mini for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer # Ensure imports\n",
    "import torch\n",
    "import os\n",
    "\n",
    "fine_tuned_phi3_full_dir = './fine_tuned_phi3_mini_summaries_full'\n",
    "fine_tuned_phi3_lora_dir = './fine_tuned_phi3_mini_summaries_lora' # Placeholder for LoRA path\n",
    "inference_model_phi3 = None\n",
    "inference_tokenizer_phi3 = None\n",
    "\n",
    "# Ensure device_phi3 is available (defined in an early cell)\n",
    "if 'device_phi3' not in globals():\n",
    "    accelerator_inf = Accelerator() # Re-init accelerator if needed for device context\n",
    "    device_phi3 = accelerator_inf.device\n",
    "    print(f\"Re-initialized device for inference: {device_phi3}\")\n",
    "\n",
    "phi3_full_finetune_was_successful = globals().get('phi3_full_finetune_successful', False)\n",
    "\n",
    "if phi3_full_finetune_was_successful and os.path.exists(fine_tuned_phi3_full_dir):\n",
    "    print(f\"Loading fully fine-tuned Phi-3 model from {fine_tuned_phi3_full_dir}...\")\n",
    "    try:\n",
    "        inference_model_phi3 = AutoModelForCausalLM.from_pretrained(fine_tuned_phi3_full_dir, trust_remote_code=True, torch_dtype=\"auto\")\n",
    "        inference_tokenizer_phi3 = AutoTokenizer.from_pretrained(fine_tuned_phi3_full_dir, trust_remote_code=True)\n",
    "        print(\"Successfully loaded fully fine-tuned model and tokenizer.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fully fine-tuned model: {e}. Will attempt to load base model.\")\n",
    "        inference_model_phi3 = None # Reset on error\n",
    "        inference_tokenizer_phi3 = None\n",
    "else:\n",
    "    print(\"Fully fine-tuned Phi-3 model not found or training was not successful.\")\n",
    "\n",
    "# Placeholder for LoRA loading - this part would need to be fleshed out if LoRA training was actually implemented\n",
    "# if inference_model_phi3 is None and os.path.exists(fine_tuned_phi3_lora_dir):\n",
    "#     print(f\"Attempting to load base model and LoRA adapters from {fine_tuned_phi3_lora_dir}...\")\n",
    "#     try:\n",
    "#         from peft import PeftModel # Requires peft library\n",
    "#         if 'phi3_model_id' not in globals(): phi3_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "#         base_model_for_lora_inf = AutoModelForCausalLM.from_pretrained(phi3_model_id, trust_remote_code=True, torch_dtype=\"auto\")\n",
    "#         inference_model_phi3 = PeftModel.from_pretrained(base_model_for_lora_inf, fine_tuned_phi3_lora_dir)\n",
    "#         inference_tokenizer_phi3 = AutoTokenizer.from_pretrained(fine_tuned_phi3_lora_dir, trust_remote_code=True)\n",
    "#         print(\"Successfully loaded base model with LoRA adapters.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading LoRA model: {e}. Will attempt to load base model.\")\n",
    "#         inference_model_phi3 = None\n",
    "#         inference_tokenizer_phi3 = None\n",
    "\n",
    "if inference_model_phi3 is None: # Fallback to base model if no fine-tuned version loaded\n",
    "    print(\"Loading base Phi-3 model for inference as no fine-tuned version was loaded.\")\n",
    "    try:\n",
    "        if 'phi3_model_id' not in globals(): phi3_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "        inference_model_phi3 = AutoModelForCausalLM.from_pretrained(phi3_model_id, trust_remote_code=True, torch_dtype=\"auto\")\n",
    "        inference_tokenizer_phi3 = AutoTokenizer.from_pretrained(phi3_model_id, trust_remote_code=True)\n",
    "        # Ensure pad token is set for base model tokenizer as well\n",
    "        if inference_tokenizer_phi3.pad_token is None:\n",
    "            inference_tokenizer_phi3.pad_token = inference_tokenizer_phi3.eos_token\n",
    "            print(f\"Set pad_token for base Phi-3 tokenizer to: '{inference_tokenizer_phi3.pad_token}'\")\n",
    "        print(\"Base Phi-3 model and tokenizer loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading base Phi-3 model: {e}\")\n",
    "\n",
    "if inference_model_phi3 and inference_tokenizer_phi3:\n",
    "    inference_model_phi3 = inference_model_phi3.to(device_phi3)\n",
    "    inference_model_phi3.eval()\n",
    "    print(f\"Phi-3 inference model is on device: {inference_model_phi3.device} and in eval mode.\")\n",
    "else:\n",
    "    print(\"Could not load any model for Phi-3 inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with Fine-tuned Phi-3 Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation quality and style will depend heavily on the success and extent of fine-tuning. If using an instruct model like Phi-3-mini-instruct, applying the correct chat template during inference (and possibly during fine-tuning) can be crucial for optimal results, especially if the fine-tuning data was not formatted that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_phi3(prompt_text, model, tokenizer, device, max_new_tokens=100):\n",
    "    if not model or not tokenizer:\n",
    "        print(\"Inference model or tokenizer for Phi-3 not available.\")\n",
    "        return []\n",
    "    try:\n",
    "        # For Phi-3 instruct, the recommended way is using the chat template if available and appropriate.\n",
    "        # However, since we fine-tuned on plain summaries, direct encoding might be what we test first.\n",
    "        # If fine-tuning was on formatted prompts, this part needs to match that format.\n",
    "        # messages = [{ \"role\": \"user\", \"content\": prompt_text }]\n",
    "        # formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        # input_ids = tokenizer.encode(formatted_prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        \n",
    "        # Using direct encoding as per our fine-tuning data format (plain summaries)\n",
    "        input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            # num_beams=5, # Optional: Beam search\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7, \n",
    "            top_p=0.9,       \n",
    "            do_sample=True   # Enable sampling for temperature/top_p\n",
    "        )\n",
    "        \n",
    "        # Decode only the newly generated tokens, not the prompt\n",
    "        generated_text = tokenizer.decode(output_sequences[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "        return [generated_text] # Return as a list to match other generation functions\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Phi-3 text generation: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'inference_model_phi3' in globals() and inference_model_phi3 and \\ \n",
    "   'inference_tokenizer_phi3' in globals() and inference_tokenizer_phi3 and \\ \n",
    "   'device_phi3' in globals():\n",
    "    \n",
    "    prompts_for_phi3 = [\n",
    "        \"This video summary is about\", \n",
    "        \"Key takeaways include\", \n",
    "        \"This video explains how to\",\n",
    "        \"An interesting point made was about\",\n",
    "        \"To learn more about this, you should\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n--- Phi-3 Mini (Summary-Tuned) Text Generation Demo ---\")\n",
    "    for p in prompts_for_phi3:\n",
    "        generated_texts_phi3 = generate_text_phi3(\n",
    "            p, \n",
    "            inference_model_phi3, \n",
    "            inference_tokenizer_phi3, \n",
    "            device_phi3, \n",
    "            max_new_tokens=75 # Keep generated sequence length reasonable for summary-like output\n",
    "        )\n",
    "        print(f\"\\nPrompt: {p}...\")\n",
    "        if generated_texts_phi3:\n",
    "            for i, g in enumerate(generated_texts_phi3):\n",
    "                print(f\"Generated {i+1}: {g}\")\n",
    "        else:\n",
    "            print(\"  (No text generated or error occurred)\")\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"Phi-3 inference model, tokenizer, or device not loaded/defined. Cannot demonstrate generation.\")"
   ]
  }
 ]
}
