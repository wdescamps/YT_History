{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Semantic Analysis of YouTube Watch History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the cleaned data file\n",
    "cleaned_data_path = '../data/cleaned_watch_history.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(cleaned_data_path, parse_dates=['timestamp_utc'])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {cleaned_data_path} was not found. Please ensure the previous notebook ran successfully and created this file.\")\n",
    "    # Create an empty DataFrame with expected columns to avoid errors in subsequent cells if file not found\n",
    "    df = pd.DataFrame(columns=['title', 'video_url', 'channel_name', 'timestamp_utc'])\n",
    "    df['timestamp_utc'] = pd.to_datetime(df['timestamp_utc'])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"DataFrame head:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display DataFrame information\n",
    "print(\"\\nDataFrame info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a pleasant style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Top Watched Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'channel_name' in df.columns and not df['channel_name'].isnull().all():\n",
    "    top_n_channels = 15\n",
    "    channel_counts = df['channel_name'].value_counts().nlargest(top_n_channels)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=channel_counts.values, y=channel_counts.index, palette='viridis')\n",
    "    plt.title(f'Top {top_n_channels} Most Watched Channels')\n",
    "    plt.xlabel('Number of Videos Watched')\n",
    "    plt.ylabel('Channel Name')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"DataFrame is empty, 'channel_name' column is missing, or all channel names are NaN. Skipping Top Watched Channels plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Viewing Activity Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'timestamp_utc' in df.columns and pd.api.types.is_datetime64_any_dtype(df['timestamp_utc']) and not df['timestamp_utc'].isnull().all():\n",
    "    # Videos per month\n",
    "    df['watch_month_year'] = df['timestamp_utc'].dt.to_period('M')\n",
    "    videos_per_month = df.groupby('watch_month_year').size()\n",
    "    videos_per_month.index = videos_per_month.index.astype(str)\n",
    "    videos_per_month = videos_per_month.sort_index()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    videos_per_month.plot(kind='bar', colormap='cividis')\n",
    "    plt.title('Videos Watched per Month-Year')\n",
    "    plt.xlabel('Month-Year')\n",
    "    plt.ylabel('Number of Videos Watched')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"DataFrame is empty or 'timestamp_utc' column is missing, not datetime, or all values are NaT. Skipping Videos per month plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'timestamp_utc' in df.columns and pd.api.types.is_datetime64_any_dtype(df['timestamp_utc']) and not df['timestamp_utc'].isnull().all():\n",
    "    # Videos per hour of the day\n",
    "    df['watch_hour'] = df['timestamp_utc'].dt.hour\n",
    "    videos_per_hour = df.groupby('watch_hour').size().reindex(range(24), fill_value=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    videos_per_hour.plot(kind='bar', colormap='plasma')\n",
    "    plt.title('Videos Watched per Hour of the Day')\n",
    "    plt.xlabel('Hour of the Day (0-23)')\n",
    "    plt.ylabel('Number of Videos Watched')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"DataFrame is empty or 'timestamp_utc' column is missing, not datetime, or all values are NaT. Skipping Videos per hour plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'timestamp_utc' in df.columns and pd.api.types.is_datetime64_any_dtype(df['timestamp_utc']) and not df['timestamp_utc'].isnull().all():\n",
    "    # Videos per day of the week\n",
    "    df['watch_dayofweek_name'] = df['timestamp_utc'].dt.day_name()\n",
    "    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    videos_per_day = df.groupby('watch_dayofweek_name').size().reindex(days_order, fill_value=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    videos_per_day.plot(kind='bar', colormap='summer')\n",
    "    plt.title('Videos Watched per Day of the Week')\n",
    "    plt.xlabel('Day of the Week')\n",
    "    plt.ylabel('Number of Videos Watched')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"DataFrame is empty or 'timestamp_utc' column is missing, not datetime, or all values are NaT. Skipping Videos per day of week plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Semantic Analysis of Video Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download nltk resources (run once)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "try:\n",
    "    word_tokenize('test')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = str(text).lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in punctuations and word.isalpha()] # Keep only alphabetic tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "if not df.empty and 'title' in df.columns:\n",
    "    df['cleaned_title_tokens'] = df['title'].apply(preprocess_text)\n",
    "    df['cleaned_title'] = df['cleaned_title_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "    print(\"Sample cleaned titles:\")\n",
    "    print(df[['title', 'cleaned_title']].head())\n",
    "else:\n",
    "    print(\"DataFrame is empty or 'title' column is missing. Skipping text preprocessing.\")\n",
    "    # Ensure columns exist even if processing is skipped to prevent errors in later cells\n",
    "    if 'cleaned_title_tokens' not in df.columns:\n",
    "        df['cleaned_title_tokens'] = pd.Series(dtype='object')\n",
    "    if 'cleaned_title' not in df.columns:\n",
    "        df['cleaned_title'] = pd.Series(dtype='str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_matrix = None # Initialize to None\n",
    "vectorizer = None # Initialize to None\n",
    "\n",
    "if not df.empty and 'cleaned_title' in df.columns and df['cleaned_title'].astype(bool).any(): # Check if there's any actual text data\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(df['cleaned_title'].dropna()) # Drop NA to be safe\n",
    "        print(f\"Shape of TF-IDF matrix: {tfidf_matrix.shape}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during TF-IDF vectorization: {e}. This might happen if 'cleaned_title' is all empty strings or NaNs.\")\n",
    "else:\n",
    "    print(\"DataFrame is empty, 'cleaned_title' column is missing, or contains no text data. Skipping TF-IDF Vectorization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \" + \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "if tfidf_matrix is not None and vectorizer is not None:\n",
    "    n_topics = 5 # Define number of topics\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    \n",
    "    # Check if tfidf_matrix has any non-zero rows/documents\n",
    "    if tfidf_matrix.shape[0] > 0 and tfidf_matrix.nnz > 0:\n",
    "        lda.fit(tfidf_matrix)\n",
    "        print(f\"\\nTop words for {n_topics} topics found by LDA:\")\n",
    "        display_topics(lda, vectorizer.get_feature_names_out(), 10)\n",
    "    else:\n",
    "        print(\"TF-IDF matrix is empty or all zero. Skipping LDA fitting. This might be due to no text data or all text data being filtered out.\")\n",
    "else:\n",
    "    print(\"TF-IDF matrix or vectorizer is not available. Skipping Topic Modeling with LDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
