{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-tuning on YouTube Video Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell should be run if 'youtube-transcript-api' is not already installed in your environment.\n",
    "!pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptForLanguage, NoTranscriptFound\n",
    "import json\n",
    "import os\n",
    "import re # For basic cleaning\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Video Watch History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_path = '../data/cleaned_watch_history.csv'\n",
    "try:\n",
    "    df_history = pd.read_csv(cleaned_data_path, parse_dates=['timestamp_utc'])\n",
    "    print(f\"Successfully loaded {cleaned_data_path}\")\n",
    "    print(f\"DataFrame shape: {df_history.shape}\")\n",
    "    # print(df_history.head()) # Keep output concise for now\n",
    "    # df_history.info() # Keep output concise for now\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {cleaned_data_path} was not found. Please ensure 01_data_cleaning.ipynb ran.\")\n",
    "    df_history = pd.DataFrame(columns=['title', 'video_url', 'channel_name', 'timestamp_utc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Video Transcripts (Limited Subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VIDEOS_TO_PROCESS = 5 # Limit for initial run to be quick and respectful of APIs\n",
    "transcript_data = []\n",
    "\n",
    "def get_video_id(url):\n",
    "    \"\"\"Extracts video ID from YouTube URL.\"\"\"\n",
    "    if pd.isna(url): return None\n",
    "    if 'watch?v=' in url:\n",
    "        return url.split('watch?v=')[-1].split('&')[0]\n",
    "    elif 'youtu.be/' in url:\n",
    "        return url.split('youtu.be/')[-1].split('?')[0]\n",
    "    return None\n",
    "\n",
    "if not df_history.empty:\n",
    "    print(f\"Starting transcript fetching for up to {MAX_VIDEOS_TO_PROCESS} videos...\")\n",
    "    for index, row in df_history.head(MAX_VIDEOS_TO_PROCESS).iterrows():\n",
    "        video_url = row.get('video_url')\n",
    "        title = row.get('title', 'N/A') \n",
    "        video_id = get_video_id(video_url)\n",
    "\n",
    "        if not video_id:\n",
    "            print(f\"Could not extract video_id from URL: {video_url}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing video: {title} (ID: {video_id})\")\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "            target_languages = ['en', 'fr'] \n",
    "            fetched_transcript_obj = None\n",
    "            for lang in target_languages:\n",
    "                try:\n",
    "                    fetched_transcript_obj = transcript_list.find_manually_created_transcript([lang])\n",
    "                    print(f\"Found manually created transcript in '{lang}' for {video_id}\")\n",
    "                    break\n",
    "                except NoTranscriptFound:\n",
    "                    continue\n",
    "            if not fetched_transcript_obj:\n",
    "                for lang in target_languages:\n",
    "                    try:\n",
    "                        fetched_transcript_obj = transcript_list.find_generated_transcript([lang])\n",
    "                        print(f\"Found auto-generated transcript in '{lang}' for {video_id}\")\n",
    "                        break\n",
    "                    except NoTranscriptFound:\n",
    "                        continue\n",
    "            if fetched_transcript_obj:\n",
    "                full_transcript_text = \" \".join([segment['text'] for segment in fetched_transcript_obj.fetch()])\n",
    "                full_transcript_text = re.sub(r'\\n+', ' ', full_transcript_text)\n",
    "                full_transcript_text = re.sub(r'\\s+', ' ', full_transcript_text).strip()\n",
    "                transcript_data.append({'video_id': video_id, 'title': title, 'transcript': full_transcript_text, 'language': fetched_transcript_obj.language})\n",
    "                print(f\"Successfully fetched transcript for {video_id} (Language: {fetched_transcript_obj.language})\")\n",
    "            else:\n",
    "                print(f\"No suitable transcript found for {video_id} in languages {target_languages}\")\n",
    "        except TranscriptsDisabled:\n",
    "            print(f\"Transcripts are disabled for video {video_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred for video {video_id}: {e}\")\n",
    "        time.sleep(0.5)\n",
    "    print(\"\\nFinished transcript fetching process.\")\n",
    "else:\n",
    "    print(\"Watch history DataFrame is empty. No transcripts to fetch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Collected Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_transcripts_file = '../data/video_transcripts.jsonl'\n",
    "processed_count = 0\n",
    "try:\n",
    "    with open(output_transcripts_file, 'w', encoding='utf-8') as f:\n",
    "        for entry in transcript_data:\n",
    "            if isinstance(entry, dict):\n",
    "                json.dump(entry, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "                processed_count +=1\n",
    "    print(f\"Saved {processed_count} transcripts to {output_transcripts_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving transcripts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "input_transcripts_file = '../data/video_transcripts.jsonl'\n",
    "loaded_transcripts_list = []\n",
    "try:\n",
    "    with open(input_transcripts_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            loaded_transcripts_list.append(json.loads(line))\n",
    "    df_transcripts = pd.DataFrame(loaded_transcripts_list)\n",
    "    print(f\"Successfully loaded {len(df_transcripts)} transcripts from {input_transcripts_file}.\")\n",
    "    # print(df_transcripts.head()) # Concise\n",
    "    # df_transcripts.info() # Concise\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Transcript file {input_transcripts_file} not found. Please run the fetching part first.\")\n",
    "    df_transcripts = pd.DataFrame(columns=['video_id', 'title', 'transcript', 'language']) \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON from {input_transcripts_file}: {e}\")\n",
    "    df_transcripts = pd.DataFrame(columns=['video_id', 'title', 'transcript', 'language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Transcript Text for GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "chunked_input_ids = [] \n",
    "tokenizer_gpt2_transcripts = None # Initialize\n",
    "\n",
    "if not df_transcripts.empty and 'transcript' in df_transcripts.columns:\n",
    "    df_transcripts_valid = df_transcripts[df_transcripts['transcript'].str.len() > 50].copy()\n",
    "    print(f\"Number of transcripts after filtering short ones: {len(df_transcripts_valid)}\")\n",
    "\n",
    "    if not df_transcripts_valid.empty:\n",
    "        full_text_corpus = \"\\n\\n\".join(df_transcripts_valid['transcript'].tolist())\n",
    "        try:\n",
    "            tokenizer_gpt2_transcripts = AutoTokenizer.from_pretrained('gpt2')\n",
    "            if tokenizer_gpt2_transcripts.pad_token is None:\n",
    "                tokenizer_gpt2_transcripts.pad_token = tokenizer_gpt2_transcripts.eos_token\n",
    "                print(f\"Set pad_token for GPT-2 transcript tokenizer to: {tokenizer_gpt2_transcripts.eos_token}\")\n",
    "\n",
    "            tokenized_corpus = tokenizer_gpt2_transcripts.encode(full_text_corpus)\n",
    "            print(f\"Total tokens in corpus: {len(tokenized_corpus)}\")\n",
    "\n",
    "            max_seq_length = 128 \n",
    "            for i in range(0, len(tokenized_corpus) - max_seq_length + 1, max_seq_length):\n",
    "                chunked_input_ids.append(tokenized_corpus[i : i + max_seq_length])\n",
    "            print(f\"Number of chunks created: {len(chunked_input_ids)}\")\n",
    "            if chunked_input_ids:\n",
    "                print(f\"Example chunk (first 10 tokens): {chunked_input_ids[0][:10]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization or chunking: {e}\")\n",
    "    else:\n",
    "        print(\"No valid transcripts long enough for processing after filtering.\")\n",
    "else:\n",
    "    print(\"Transcript DataFrame is empty or 'transcript' column is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset and DataLoader for Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_transcripts_gpt2 = None\n",
    "dataloader_transcripts_gpt2 = None\n",
    "\n",
    "class TranscriptDatasetGPT2(Dataset):\n",
    "    def __init__(self, chunked_input_ids):\n",
    "        self.chunked_input_ids = chunked_input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunked_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunked_input_ids[idx]\n",
    "        return {'input_ids': torch.tensor(chunk, dtype=torch.long), \n",
    "                'labels': torch.tensor(chunk, dtype=torch.long).clone()}\n",
    "\n",
    "if chunked_input_ids: \n",
    "    dataset_transcripts_gpt2 = TranscriptDatasetGPT2(chunked_input_ids)\n",
    "    print(f\"Created transcript dataset with {len(dataset_transcripts_gpt2)} samples (chunks).\")\n",
    "\n",
    "    if len(dataset_transcripts_gpt2) > 0:\n",
    "        dataloader_transcripts_gpt2 = DataLoader(dataset_transcripts_gpt2, batch_size=1, shuffle=True)\n",
    "        print(f\"Created transcript DataLoader with batch_size=1.\")\n",
    "    else:\n",
    "        print(\"Dataset for transcripts is empty, DataLoader not created.\")\n",
    "else:\n",
    "    print(\"No chunked input_ids available. Cannot create Dataset/DataLoader for transcripts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration for Apple Silicon (MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # Should be already imported, but good for cell independence\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device_transcripts = torch.device(\"mps\")\n",
    "    print(\"MPS device found for transcript training. Using MPS.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device_transcripts = torch.device(\"cuda\")\n",
    "    print(\"CUDA device found for transcript training. Using CUDA.\")\n",
    "else:\n",
    "    device_transcripts = torch.device(\"cpu\")\n",
    "    print(\"MPS or CUDA not available for transcript training. Using CPU.\")\n",
    "print(f\"Selected device for transcript training: {device_transcripts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained GPT-2 Model for Transcript Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name_gpt2_transcripts = 'gpt2' # Standard GPT-2 model\n",
    "model_gpt2_transcripts = None\n",
    "\n",
    "try:\n",
    "    model_gpt2_transcripts = AutoModelForCausalLM.from_pretrained(model_name_gpt2_transcripts)\n",
    "    if 'device_transcripts' in globals(): # Check if device_transcripts is defined\n",
    "        model_gpt2_transcripts.to(device_transcripts)\n",
    "        print(f\"Pre-trained model '{model_name_gpt2_transcripts}' loaded and moved to {device_transcripts}.\")\n",
    "    else:\n",
    "        print(f\"Pre-trained model '{model_name_gpt2_transcripts}' loaded, but device_transcripts not defined. Model on CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pre-trained model '{model_name_gpt2_transcripts}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning GPT-2 on Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "gpt2_transcript_fine_tuning_done = False\n",
    "if 'device_transcripts' in globals() and \\ \n",
    "   'model_gpt2_transcripts' in globals() and model_gpt2_transcripts is not None and \\ \n",
    "   'dataloader_transcripts_gpt2' in globals() and dataloader_transcripts_gpt2 is not None:\n",
    "\n",
    "    print(f\"Starting fine-tuning of GPT-2 on transcripts using {device_transcripts}...\")\n",
    "    optimizer_gpt2_transcripts = AdamW(model_gpt2_transcripts.parameters(), lr=5e-5)\n",
    "    num_epochs_gpt2_transcripts = 1 # Critical to keep low for large transcript data & limited time/compute\n",
    "\n",
    "    model_gpt2_transcripts.train()\n",
    "    for epoch in range(num_epochs_gpt2_transcripts):\n",
    "        total_loss_transcripts = 0\n",
    "        print(f\"Starting Epoch {epoch+1}/{num_epochs_gpt2_transcripts}\")\n",
    "        for batch_idx, batch in enumerate(dataloader_transcripts_gpt2):\n",
    "            optimizer_gpt2_transcripts.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device_transcripts)\n",
    "            labels = batch['labels'].to(device_transcripts)\n",
    "            \n",
    "            outputs = model_gpt2_transcripts(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                optimizer_gpt2_transcripts.step()\n",
    "                total_loss_transcripts += loss.item()\n",
    "                if batch_idx % 20 == 0: # Log progress every 20 batches\n",
    "                   print(f\"  Epoch {epoch+1}, Batch {batch_idx}/{len(dataloader_transcripts_gpt2)}, Loss: {loss.item():.4f}\")\n",
    "            else:\n",
    "                print(f\"Warning: Loss is None for batch {batch_idx}. Skipping.\")\n",
    "\n",
    "        avg_loss_transcripts = total_loss_transcripts / len(dataloader_transcripts_gpt2) if len(dataloader_transcripts_gpt2) > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs_gpt2_transcripts} - Average Training Loss: {avg_loss_transcripts:.4f}\")\n",
    "    \n",
    "    print(\"Fine-tuning GPT-2 on transcripts completed.\")\n",
    "    gpt2_transcript_fine_tuning_done = True\n",
    "else:\n",
    "    print(\"Required variables (device, model, dataloader for transcripts) not available. Skipping fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Transcript-Tuned GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Ensure os is imported\n",
    "output_dir_gpt2_transcripts = './fine_tuned_gpt2_youtube_transcripts'\n",
    "\n",
    "if 'gpt2_transcript_fine_tuning_done' in globals() and gpt2_transcript_fine_tuning_done and \\ \n",
    "   'model_gpt2_transcripts' in globals() and model_gpt2_transcripts is not None and \\ \n",
    "   'tokenizer_gpt2_transcripts' in globals() and tokenizer_gpt2_transcripts is not None:\n",
    "    try:\n",
    "        os.makedirs(output_dir_gpt2_transcripts, exist_ok=True)\n",
    "        model_gpt2_transcripts.save_pretrained(output_dir_gpt2_transcripts)\n",
    "        tokenizer_gpt2_transcripts.save_pretrained(output_dir_gpt2_transcripts)\n",
    "        print(f\"Transcript-tuned GPT-2 Model and tokenizer saved to {output_dir_gpt2_transcripts}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving transcript-tuned GPT-2 model/tokenizer: {e}\")\n",
    "elif not ('gpt2_transcript_fine_tuning_done' in globals() and gpt2_transcript_fine_tuning_done):\n",
    "    print(\"Transcript fine-tuning was not performed or completed. Skipping saving model.\")\n",
    "else:\n",
    "    print(\"Transcript-tuned GPT-2 Model or its Tokenizer not available. Skipping saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Transcript-Tuned GPT-2 for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer # Ensure imports\n",
    "import torch\n",
    "import os\n",
    "\n",
    "fine_tuned_output_dir_transcripts = './fine_tuned_gpt2_youtube_transcripts' # Matches saving directory\n",
    "inference_model_transcripts = None\n",
    "inference_tokenizer_transcripts = None\n",
    "\n",
    "# Ensure device_transcripts is available from its definition cell\n",
    "if 'device_transcripts' not in globals():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device_transcripts = torch.device(\"mps\")\n",
    "        print(\"Re-initialized MPS device for inference.\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device_transcripts = torch.device(\"cuda\")\n",
    "        print(\"Re-initialized CUDA device for inference.\")\n",
    "    else:\n",
    "        device_transcripts = torch.device(\"cpu\")\n",
    "        print(\"Re-initialized CPU device for inference.\")\n",
    "\n",
    "if os.path.exists(fine_tuned_output_dir_transcripts):\n",
    "    try:\n",
    "        inference_model_transcripts = AutoModelForCausalLM.from_pretrained(fine_tuned_output_dir_transcripts)\n",
    "        inference_tokenizer_transcripts = AutoTokenizer.from_pretrained(fine_tuned_output_dir_transcripts)\n",
    "        print(f\"Fine-tuned transcript model and tokenizer loaded from {fine_tuned_output_dir_transcripts}.\")\n",
    "        \n",
    "        inference_model_transcripts.to(device_transcripts)\n",
    "        inference_model_transcripts.eval()\n",
    "        print(f\"Transcript inference model moved to {device_transcripts} and set to eval mode.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned transcript model or tokenizer: {e}\")\n",
    "else:\n",
    "    print(f\"Directory not found: {fine_tuned_output_dir_transcripts}. Ensure model was saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with Transcript-Tuned GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_transcripts_model(prompt, model, tokenizer, device, max_length=100, num_return_sequences=1):\n",
    "    if not model or not tokenizer:\n",
    "        print(\"Inference model or tokenizer for transcripts not available.\")\n",
    "        return []\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2, \n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7, # Add some creativity\n",
    "            top_k=50         # Top-k sampling\n",
    "        )\n",
    "        \n",
    "        generated_texts = [tokenizer.decode(seq, skip_special_tokens=True) for seq in output_sequences]\n",
    "        return generated_texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text generation with transcript model: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'inference_model_transcripts' in globals() and inference_model_transcripts and \\ \n",
    "   'inference_tokenizer_transcripts' in globals() and inference_tokenizer_transcripts and \\ \n",
    "   'device_transcripts' in globals():\n",
    "    \n",
    "    prompts_for_transcripts = [\n",
    "        \"Today we're going to talk about\", \n",
    "        \"The key thing to remember is\", \n",
    "        \"What if I told you that\",\n",
    "        \"In this video, I will show you\",\n",
    "        \"Let's dive into the world of\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n--- Transcript-Tuned GPT-2 Text Generation Demo ---\")\n",
    "    for p in prompts_for_transcripts:\n",
    "        generated_texts = generate_text_from_transcripts_model(\n",
    "            p, \n",
    "            inference_model_transcripts, \n",
    "            inference_tokenizer_transcripts, \n",
    "            device_transcripts, \n",
    "            max_length=75 # Keep generated sequence length reasonable\n",
    "        )\n",
    "        print(f\"\\nPrompt: {p}...\")\n",
    "        if generated_texts:\n",
    "            for i, g in enumerate(generated_texts):\n",
    "                print(f\"Generated {i+1}: {g}\")\n",
    "        else:\n",
    "            print(\"  (No text generated or error occurred)\")\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"Transcript-tuned inference model, tokenizer, or device not loaded/defined. Cannot demonstrate generation.\")"
   ]
  }
 ]
}
