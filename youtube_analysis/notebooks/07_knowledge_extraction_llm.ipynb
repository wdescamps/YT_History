{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-tuning on Extracted Knowledge (Summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure transformers and torch are installed via pip if not already.\n",
    "# These should be available if previous notebooks (03-06) were run with their setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer # For summarization\n",
    "from transformers import AutoModelForCausalLM # For GPT-2 fine-tuning on summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration for Apple Silicon (MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'device_summaries' not in globals(): # Define if not already defined by a previous run of this cell\n",
    "    if torch.backends.mps.is_available():\n",
    "        device_summaries = torch.device(\"mps\")\n",
    "        print(\"MPS device found for summaries notebook. Using MPS.\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device_summaries = torch.device(\"cuda\")\n",
    "        print(\"CUDA device found for summaries notebook. Using CUDA.\")\n",
    "    else:\n",
    "        device_summaries = torch.device(\"cpu\")\n",
    "        print(\"MPS or CUDA not available for summaries notebook. Using CPU.\")\n",
    "    print(f\"Selected device for summaries notebook: {device_summaries}\")\n",
    "else:\n",
    "    print(f\"Device for summaries notebook already defined: {device_summaries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Video Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_transcripts_file = '../data/video_transcripts.jsonl'\n",
    "loaded_transcripts_list = []\n",
    "try:\n",
    "    with open(input_transcripts_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            loaded_transcripts_list.append(json.loads(line))\n",
    "    df_transcripts = pd.DataFrame(loaded_transcripts_list)\n",
    "    if not df_transcripts.empty:\n",
    "        print(f\"Successfully loaded {len(df_transcripts)} transcripts from {input_transcripts_file}.\")\n",
    "    else:\n",
    "        print(f\"Loaded transcript file {input_transcripts_file}, but it resulted in an empty DataFrame.\")\n",
    "        df_transcripts = pd.DataFrame(columns=['video_id', 'title', 'transcript', 'language_code'])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Transcript file {input_transcripts_file} not found. Please run notebook 06 first.\")\n",
    "    df_transcripts = pd.DataFrame(columns=['video_id', 'title', 'transcript', 'language_code'])\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON from {input_transcripts_file}: {e}\")\n",
    "    df_transcripts = pd.DataFrame(columns=['video_id', 'title', 'transcript', 'language_code'])\n",
    "\n",
    "if df_transcripts.empty:\n",
    "    print(\"\\nWarning: No transcripts loaded. This notebook relies on transcript data for summarization and subsequent LLM fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript Summarization using Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VIDEOS_TO_SUMMARIZE = 3\n",
    "summarizer_model = None\n",
    "summarizer_tokenizer = None\n",
    "\n",
    "if 'df_transcripts' in globals() and not df_transcripts.empty:\n",
    "    summarization_subset_df = df_transcripts.head(MAX_VIDEOS_TO_SUMMARIZE).copy()\n",
    "    print(f\"Selected {len(summarization_subset_df)} videos for summarization.\")\n",
    "\n",
    "    if not summarization_subset_df.empty:\n",
    "        summarizer_model_name = 'sshleifer/distilbart-cnn-12-6'\n",
    "        try:\n",
    "            summarizer_tokenizer = AutoTokenizer.from_pretrained(summarizer_model_name)\n",
    "            summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(summarizer_model_name)\n",
    "            if 'device_summaries' in globals():\n",
    "                summarizer_model.to(device_summaries)\n",
    "                print(f\"Summarizer model '{summarizer_model_name}' loaded and moved to {device_summaries}.\")\n",
    "            else:\n",
    "                print(f\"Summarizer model '{summarizer_model_name}' loaded, but device_summaries not defined. Model on CPU.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading summarization model/tokenizer: {e}\")\n",
    "            summarizer_model = None \n",
    "    else:\n",
    "        print(\"Subset for summarization is empty. Nothing to do.\")\n",
    "else:\n",
    "    print(\"df_transcripts is not available or empty. Skipping summarization model loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_summaries = []\n",
    "max_input_length_for_summarizer = 1024\n",
    "\n",
    "if 'summarizer_model' in globals() and summarizer_model is not None and \\ \n",
    "   'summarizer_tokenizer' in globals() and summarizer_tokenizer is not None and \\ \n",
    "   'summarization_subset_df' in globals() and not summarization_subset_df.empty and \\ \n",
    "   'device_summaries' in globals():\n",
    "    \n",
    "    print(f\"\\nStarting summarization for {len(summarization_subset_df)} transcripts...\")\n",
    "    for index, row in summarization_subset_df.iterrows():\n",
    "        video_id = row.get('video_id')\n",
    "        title = row.get('title', 'N/A')\n",
    "        transcript_text = row.get('transcript')\n",
    "\n",
    "        if not transcript_text or pd.isna(transcript_text):\n",
    "            print(f\"Skipping video_id {video_id} due to missing transcript.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Summarizing video: {title} (ID: {video_id}) Transcript length: {len(transcript_text)}\")\n",
    "        try:\n",
    "            inputs = summarizer_tokenizer.encode(\n",
    "                transcript_text, \n",
    "                return_tensors=\"pt\", \n",
    "                max_length=max_input_length_for_summarizer, \n",
    "                truncation=True\n",
    "            ).to(device_summaries)\n",
    "            \n",
    "            summary_ids = summarizer_model.generate(\n",
    "                inputs, \n",
    "                max_length=150, \n",
    "                min_length=40, \n",
    "                length_penalty=2.0, \n",
    "                num_beams=4, \n",
    "                early_stopping=True\n",
    "            )\n",
    "            summary_text = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            video_summaries.append({\n",
    "                'video_id': video_id, \n",
    "                'title': title, \n",
    "                'original_transcript_length': len(transcript_text),\n",
    "                'summary': summary_text\n",
    "            })\n",
    "            print(f\"  Successfully summarized {video_id}. Summary length: {len(summary_text)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error summarizing {video_id}: {e}\")\n",
    "    print(\"\\nSummarization process completed.\")\n",
    "else:\n",
    "    print(\"Summarization prerequisites (model, tokenizer, data, or device) not met. Skipping summary generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Video Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_summaries_file = '../data/video_summaries.jsonl'\n",
    "summaries_saved_count = 0\n",
    "if video_summaries: \n",
    "    try:\n",
    "        with open(output_summaries_file, 'w', encoding='utf-8') as f:\n",
    "            for entry in video_summaries:\n",
    "                if isinstance(entry, dict):\n",
    "                    json.dump(entry, f, ensure_ascii=False)\n",
    "                    f.write('\\n')\n",
    "                    summaries_saved_count += 1\n",
    "        print(f\"Saved {summaries_saved_count} summaries to {output_summaries_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving summaries: {e}\")\n",
    "else:\n",
    "    print(\"No summaries were generated to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Video Summaries for GPT-2 Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "input_summaries_file = '../data/video_summaries.jsonl'\n",
    "loaded_summaries_list = []\n",
    "try:\n",
    "    with open(input_summaries_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            loaded_summaries_list.append(json.loads(line))\n",
    "    df_summaries = pd.DataFrame(loaded_summaries_list)\n",
    "    if not df_summaries.empty:\n",
    "        print(f\"Successfully loaded {len(df_summaries)} summaries from {input_summaries_file}.\")\n",
    "        df_summaries.dropna(subset=['summary'], inplace=True)\n",
    "        df_summaries = df_summaries[df_summaries['summary'].str.strip() != '']\n",
    "        print(f\"Number of summaries after filtering empty/NaN: {len(df_summaries)}\")\n",
    "    else:\n",
    "        print(f\"Loaded summaries file {input_summaries_file}, but it resulted in an empty DataFrame.\")\n",
    "        df_summaries = pd.DataFrame(columns=['video_id', 'title', 'original_transcript_length', 'summary'])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Summaries file {input_summaries_file} not found. Please run the summarization part first.\")\n",
    "    df_summaries = pd.DataFrame(columns=['video_id', 'title', 'original_transcript_length', 'summary'])\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON from {input_summaries_file}: {e}\")\n",
    "    df_summaries = pd.DataFrame(columns=['video_id', 'title', 'original_transcript_length', 'summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Summaries for GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "\n",
    "encodings_summaries_input_ids = [] \n",
    "tokenizer_gpt2_summaries = None\n",
    "\n",
    "if 'df_summaries' in globals() and not df_summaries.empty and 'summary' in df_summaries.columns:\n",
    "    summary_list = df_summaries['summary'].tolist()\n",
    "    if summary_list:\n",
    "        try:\n",
    "            tokenizer_gpt2_summaries = AutoTokenizer.from_pretrained('gpt2')\n",
    "            if tokenizer_gpt2_summaries.pad_token is None:\n",
    "                tokenizer_gpt2_summaries.pad_token = tokenizer_gpt2_summaries.eos_token\n",
    "                print(f\"Set pad_token for GPT-2 summaries tokenizer to: {tokenizer_gpt2_summaries.eos_token}\")\n",
    "\n",
    "            encodings_summaries = tokenizer_gpt2_summaries(summary_list, truncation=True, padding=True, max_length=256) \n",
    "            encodings_summaries_input_ids = encodings_summaries['input_ids']\n",
    "            print(f\"Tokenized {len(encodings_summaries_input_ids)} summaries.\")\n",
    "            if encodings_summaries_input_ids:\n",
    "                print(f\"Example tokenized summary (first 10 tokens): {encodings_summaries_input_ids[0][:10]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during summary tokenization: {e}\")\n",
    "    else:\n",
    "        print(\"Summary list is empty after filtering. No summaries to tokenize.\")\n",
    "else:\n",
    "    print(\"DataFrame of summaries is empty or 'summary' column is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset and DataLoader for Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_gpt2_summaries = None\n",
    "dataloader_gpt2_summaries = None\n",
    "\n",
    "class SummaryDatasetGPT2(Dataset):\n",
    "    def __init__(self, tokenized_input_ids_list):\n",
    "        self.input_ids_list = tokenized_input_ids_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.input_ids_list[idx]\n",
    "        if not isinstance(item, list):\n",
    "            print(f\"Warning: Item at index {idx} is not a list, but {type(item)}. Skipping this item.\")\n",
    "            return {'input_ids': torch.tensor([], dtype=torch.long), \n",
    "                    'labels': torch.tensor([], dtype=torch.long)}\n",
    "        \n",
    "        return {'input_ids': torch.tensor(item, dtype=torch.long), \n",
    "                'labels': torch.tensor(item, dtype=torch.long).clone()}\n",
    "\n",
    "if 'encodings_summaries_input_ids' in globals() and encodings_summaries_input_ids: \n",
    "    dataset_gpt2_summaries = SummaryDatasetGPT2(encodings_summaries_input_ids)\n",
    "    print(f\"Created summary dataset with {len(dataset_gpt2_summaries)} samples.\")\n",
    "\n",
    "    if len(dataset_gpt2_summaries) > 0:\n",
    "        dataloader_gpt2_summaries = DataLoader(dataset_gpt2_summaries, batch_size=1, shuffle=True)\n",
    "        print(f\"Created summary DataLoader with batch_size=1.\")\n",
    "    else:\n",
    "        print(\"Summary dataset is empty. DataLoader not created.\")\n",
    "else:\n",
    "    print(\"No tokenized summaries available. Cannot create Dataset/DataLoader for summaries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained GPT-2 Model for Summary Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM \n",
    "\n",
    "model_name_gpt2_summaries = 'gpt2'\n",
    "model_gpt2_summaries = None \n",
    "\n",
    "try:\n",
    "    model_gpt2_summaries = AutoModelForCausalLM.from_pretrained(model_name_gpt2_summaries)\n",
    "    if 'device_summaries' in globals(): \n",
    "        model_gpt2_summaries.to(device_summaries)\n",
    "        print(f\"Pre-trained model '{model_name_gpt2_summaries}' loaded and moved to {device_summaries}.\")\n",
    "    else:\n",
    "        print(f\"Pre-trained model '{model_name_gpt2_summaries}' loaded, but device_summaries not defined. Model remains on CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pre-trained model '{model_name_gpt2_summaries}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning GPT-2 on Video Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW \n",
    "\n",
    "gpt2_summary_fine_tuning_done = False \n",
    "\n",
    "if 'device_summaries' in globals() and \\ \n",
    "   'model_gpt2_summaries' in globals() and model_gpt2_summaries is not None and \\ \n",
    "   'dataloader_gpt2_summaries' in globals() and dataloader_gpt2_summaries is not None:\n",
    "    \n",
    "    print(f\"Starting fine-tuning of GPT-2 on summaries using {device_summaries}...\")\n",
    "    optimizer_gpt2_summaries = AdamW(model_gpt2_summaries.parameters(), lr=5e-5)\n",
    "    num_epochs_gpt2_summaries = 3 \n",
    "\n",
    "    model_gpt2_summaries.train()\n",
    "    for epoch in range(num_epochs_gpt2_summaries):\n",
    "        total_loss_summaries = 0\n",
    "        print(f\"Starting Summary Fine-Tuning Epoch {epoch+1}/{num_epochs_gpt2_summaries}\")\n",
    "        for batch_idx, batch in enumerate(dataloader_gpt2_summaries):\n",
    "            optimizer_gpt2_summaries.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device_summaries)\n",
    "            labels = batch['labels'].to(device_summaries)\n",
    "            \n",
    "            outputs = model_gpt2_summaries(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                optimizer_gpt2_summaries.step()\n",
    "                total_loss_summaries += loss.item()\n",
    "                if batch_idx % 10 == 0: \n",
    "                   print(f\"  Epoch {epoch+1}, Batch {batch_idx}/{len(dataloader_gpt2_summaries)}, Loss: {loss.item():.4f}\")\n",
    "            else:\n",
    "                print(f\"Warning: Loss is None for summary batch {batch_idx}. Skipping.\")\n",
    "        \n",
    "        avg_loss_summaries = total_loss_summaries / len(dataloader_gpt2_summaries) if len(dataloader_gpt2_summaries) > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs_gpt2_summaries} - Average Training Loss on Summaries: {avg_loss_summaries:.4f}\")\n",
    "    \n",
    "    print(\"Fine-tuning GPT-2 on summaries completed.\")\n",
    "    gpt2_summary_fine_tuning_done = True\n",
    "else:\n",
    "    print(\"Required variables (device, model, dataloader for summaries) not available. Skipping fine-tuning on summaries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Summary-Tuned GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "output_dir_gpt2_summaries = './fine_tuned_gpt2_youtube_summaries'\n",
    "\n",
    "if 'gpt2_summary_fine_tuning_done' in globals() and gpt2_summary_fine_tuning_done and \\ \n",
    "   'model_gpt2_summaries' in globals() and model_gpt2_summaries is not None and \\ \n",
    "   'tokenizer_gpt2_summaries' in globals() and tokenizer_gpt2_summaries is not None:\n",
    "    try:\n",
    "        os.makedirs(output_dir_gpt2_summaries, exist_ok=True)\n",
    "        model_gpt2_summaries.save_pretrained(output_dir_gpt2_summaries)\n",
    "        tokenizer_gpt2_summaries.save_pretrained(output_dir_gpt2_summaries)\n",
    "        print(f\"Summary-tuned GPT-2 Model and tokenizer saved to {output_dir_gpt2_summaries}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving summary-tuned GPT-2 model/tokenizer: {e}\")\n",
    "elif not ('gpt2_summary_fine_tuning_done' in globals() and gpt2_summary_fine_tuning_done):\n",
    "    print(\"Summary fine-tuning was not performed or completed. Skipping saving model.\")\n",
    "else:\n",
    "    print(\"Summary-tuned GPT-2 Model or its Tokenizer not available. Skipping saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Summary-Tuned GPT-2 for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer # Ensure imports\n",
    "import torch\n",
    "import os\n",
    "\n",
    "fine_tuned_output_dir_summaries = './fine_tuned_gpt2_youtube_summaries' # Matches saving directory\n",
    "inference_model_summaries = None\n",
    "inference_tokenizer_summaries = None\n",
    "\n",
    "# Ensure device_summaries is available (it's defined in an early cell)\n",
    "if 'device_summaries' not in globals():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device_summaries = torch.device(\"mps\")\n",
    "        print(\"Re-initialized MPS device for inference.\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device_summaries = torch.device(\"cuda\")\n",
    "        print(\"Re-initialized CUDA device for inference.\")\n",
    "    else:\n",
    "        device_summaries = torch.device(\"cpu\")\n",
    "        print(\"Re-initialized CPU device for inference.\")\n",
    "\n",
    "if os.path.exists(fine_tuned_output_dir_summaries):\n",
    "    try:\n",
    "        inference_model_summaries = AutoModelForCausalLM.from_pretrained(fine_tuned_output_dir_summaries)\n",
    "        inference_tokenizer_summaries = AutoTokenizer.from_pretrained(fine_tuned_output_dir_summaries)\n",
    "        print(f\"Fine-tuned summary model and tokenizer loaded from {fine_tuned_output_dir_summaries}.\")\n",
    "        \n",
    "        inference_model_summaries.to(device_summaries)\n",
    "        inference_model_summaries.eval()\n",
    "        print(f\"Summary inference model moved to {device_summaries} and set to eval mode.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned summary model or tokenizer: {e}\")\n",
    "else:\n",
    "    print(f\"Directory not found: {fine_tuned_output_dir_summaries}. Ensure model was trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with Summary-Tuned GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_summary_model(prompt, model, tokenizer, device, max_length=100, num_return_sequences=1):\n",
    "    if not model or not tokenizer:\n",
    "        print(\"Inference model or tokenizer for summaries not available.\")\n",
    "        return []\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=20, # Encourage slightly longer than just a few words\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            num_beams=5, # Beam search can produce more coherent text\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.8, \n",
    "            top_k=50\n",
    "        )\n",
    "        \n",
    "        generated_texts = [tokenizer.decode(seq, skip_special_tokens=True) for seq in output_sequences]\n",
    "        return generated_texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text generation with summary-tuned model: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'inference_model_summaries' in globals() and inference_model_summaries and \\ \n",
    "   'inference_tokenizer_summaries' in globals() and inference_tokenizer_summaries and \\ \n",
    "   'device_summaries' in globals():\n",
    "    \n",
    "    prompts_for_summaries = [\n",
    "        \"The main topic of this video is\", \n",
    "        \"Key takeaways include\", \n",
    "        \"This video explains how to\",\n",
    "        \"An interesting point made was about\",\n",
    "        \"To learn more about this, you should\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n--- Summary-Tuned GPT-2 Text Generation Demo ---\")\n",
    "    for p in prompts_for_summaries:\n",
    "        generated_texts_summary_model = generate_text_from_summary_model(\n",
    "            p, \n",
    "            inference_model_summaries, \n",
    "            inference_tokenizer_summaries, \n",
    "            device_summaries, \n",
    "            max_length=75 # Keep generated sequence length reasonable for summary-like output\n",
    "        )\n",
    "        print(f\"\\nPrompt: {p}...\")\n",
    "        if generated_texts_summary_model:\n",
    "            for i, g in enumerate(generated_texts_summary_model):\n",
    "                print(f\"Generated {i+1}: {g}\")\n",
    "        else:\n",
    "            print(\"  (No text generated or error occurred)\")\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"Summary-tuned inference model, tokenizer, or device not loaded/defined. Cannot demonstrate generation.\")"
   ]
  }
 ]
}
