{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Training for Virtual Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the cleaned data file\n",
    "cleaned_data_path = '../data/cleaned_watch_history.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(cleaned_data_path, parse_dates=['timestamp_utc'])\n",
    "    print(f\"Successfully loaded {cleaned_data_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {cleaned_data_path} was not found. Please ensure the 01_data_cleaning notebook ran successfully.\")\n",
    "    df = pd.DataFrame(columns=['title', 'video_url', 'channel_name', 'timestamp_utc', 'cleaned_title'])\n",
    "    df['timestamp_utc'] = pd.to_datetime(df['timestamp_utc'])\n",
    "\n",
    "print(\"DataFrame head:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataFrame info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Preparation for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'cleaned_title' column exists. If not, run preprocessing steps.\n",
    "if 'cleaned_title' not in df.columns or df['cleaned_title'].isnull().all():\n",
    "    print(\"'cleaned_title' not found or is empty. Running preprocessing...\")\n",
    "    import nltk\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    # Ensure nltk resources are available\n",
    "    try:\n",
    "        stopwords.words('english')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "    try:\n",
    "        word_tokenize('test') # Test if punkt is available\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "\n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "    punctuations_set = string.punctuation\n",
    "\n",
    "    def preprocess_text_for_llm(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in punctuations_set and word.isalpha()] # Keep only alphabetic tokens\n",
    "        tokens = [word for word in tokens if word not in stop_words_set]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    if 'title' in df.columns:\n",
    "        df['cleaned_title'] = df['title'].apply(preprocess_text_for_llm)\n",
    "        print(\"Finished preprocessing 'title' into 'cleaned_title'.\")\n",
    "        print(df[['title', 'cleaned_title']].head())\n",
    "    else:\n",
    "        print(\"Error: 'title' column missing, cannot generate 'cleaned_title'.\")\n",
    "        # Ensure 'cleaned_title' exists with empty strings if title was missing, to prevent downstream errors\n",
    "        if 'cleaned_title' not in df.columns:\n",
    "             df['cleaned_title'] = \"\"\n",
    "else:\n",
    "    print(\"'cleaned_title' column found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased' # Example model\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Tokenizer for '{model_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "if tokenizer and 'cleaned_title' in df.columns and not df['cleaned_title'].isnull().all():\n",
    "    # Ensure there are actual text strings to tokenize\n",
    "    if df['cleaned_title'].astype(str).str.strip().any():\n",
    "        df['tokenized_input_ids'] = df['cleaned_title'].apply(\n",
    "            lambda x: tokenizer.encode(x, truncation=True, max_length=512) if pd.notna(x) and x.strip() else []\n",
    "        )\n",
    "        print(\"\\nTokenized 'cleaned_title' into 'tokenized_input_ids':\")\n",
    "        print(df[['cleaned_title', 'tokenized_input_ids']].head())\n",
    "    else:\n",
    "        print(\"'cleaned_title' column contains no actual text to tokenize. Skipping tokenization.\")\n",
    "        df['tokenized_input_ids'] = pd.Series(dtype='object') # Create empty series\n",
    "elif not tokenizer:\n",
    "    print(\"Tokenizer not loaded. Skipping tokenization.\")\n",
    "    df['tokenized_input_ids'] = pd.Series(dtype='object') # Create empty series\n",
    "else:\n",
    "    print(\"'cleaned_title' column not found or is all null. Skipping tokenization.\")\n",
    "    df['tokenized_input_ids'] = pd.Series(dtype='object') # Create empty series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Model Loading and Fine-tuning (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select an appropriate pre-trained model for sequence classification or generation\n",
    "#    (e.g., AutoModelForSequenceClassification or AutoModelForCausalLM from Hugging Face)\n",
    "# from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# 2. Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          # output directory\n",
    "#     num_train_epochs=3,              # total number of training epochs\n",
    "#     per_device_train_batch_size=8,   # batch size per device during training\n",
    "#     per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='./logs',            # directory for storing logs\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "\n",
    "# 3. Create a custom Dataset object if necessary, or prepare data in a format\n",
    "#    suitable for the Trainer API. This would involve the 'tokenized_input_ids'\n",
    "#    and appropriate labels if doing supervised fine-tuning.\n",
    "\n",
    "# 4. Initialize the Trainer\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# trainer = Trainer(\n",
    "#     model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "#     args=training_args,                  # training arguments, defined above\n",
    "#     train_dataset=your_train_dataset,    # training dataset\n",
    "#     eval_dataset=your_eval_dataset       # evaluation dataset (optional)\n",
    "# )\n",
    "\n",
    "# 5. Start fine-tuning\n",
    "# trainer.train()\n",
    "\n",
    "# 6. Evaluate the model\n",
    "# trainer.evaluate()\n",
    "\n",
    "# 7. Save the model\n",
    "# model.save_pretrained('./fine_tuned_youtube_llm')\n",
    "# tokenizer.save_pretrained('./fine_tuned_youtube_llm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
