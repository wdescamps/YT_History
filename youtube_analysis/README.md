# YouTube Watch History Analysis for Portfolio Project

This project analyzes YouTube watch history data to demonstrate skills in data cleaning, semantic analysis, and preparation for Large Language Model (LLM) fine-tuning.

## Project Structure

- `data/`: Contains the raw `watch-history.html` (user-provided) and the `cleaned_watch_history.csv` generated by the first notebook. It will also store `video_transcripts.jsonl` (from notebook 06) and `video_summaries.jsonl` (from notebook 07) if those notebooks are run.
- `notebooks/`: Contains the Jupyter notebooks for each stage of the analysis.
    - `01_data_cleaning.ipynb`: Loads the raw `watch-history.html`, parses it, cleans the data (timestamps, duplicates, etc.), and saves the result to `cleaned_watch_history.csv`.
    - `02_semantic_analysis.ipynb`: Performs exploratory data analysis (EDA) on the cleaned watch history, including identifying top channels and viewing patterns. It also conducts semantic analysis on video titles using TF-IDF and Latent Dirichlet Allocation (LDA) for topic modeling.
    - `03_llm_training.ipynb`: Prepares the video title data for potential fine-tuning of a Large Language Model. This includes tokenization using Hugging Face `transformers` and provides a commented outline for the model training process.
    - `04_lightweight_llm_training.ipynb`: Demonstrates fine-tuning a lightweight Large Language Model (DistilGPT2) on the cleaned video titles for text generation. It includes data preparation, a basic training loop (CPU-based), model saving, and an example of generating new text based on prompts.
    - `05_advanced_llm_training_m1.ipynb`: Implements fine-tuning for a more powerful LLM (GPT-2, 124M parameters) specifically targeting Apple Silicon (M1/M2/M3) Macs with MPS for GPU acceleration. It includes device checking, data preparation, a training loop configured for MPS, model saving, and text generation. This notebook demonstrates a more resource-intensive training process compared to the DistilGPT2 example.
    - `06_transcript_llm_training.ipynb`: An advanced notebook that fetches full video transcripts using `youtube_transcript_api.get_transcript(video_id, languages=['en', 'fr'])` (attempting to retrieve them in preferred languages like English or French) and then fine-tunes GPT-2 on this data. It includes transcript fetching with error handling, data preparation for long text, and fine-tuning/inference on an MPS-enabled device. **This notebook is highly experimental and resource-intensive.** It's crucial to start with a very small number of videos (by adjusting `MAX_VIDEOS_TO_PROCESS` within the notebook).
    - `07_knowledge_extraction_llm.ipynb`: Explores a knowledge extraction approach by first summarizing video transcripts and then fine-tuning GPT-2 on these summaries.
        - Stage 1: Uses a pre-trained summarization model (e.g., DistilBART) to generate summaries from a small subset of transcripts (fetched by notebook 06).
        - Stage 2: Fine-tunes GPT-2 on these generated summaries, targeting MPS for Apple Silicon acceleration.
        - Includes inference with this "knowledge-tuned" model.
        - **This notebook is highly experimental and involves multiple lengthy processing steps (summarization and LLM fine-tuning).** It's critical to use the `MAX_VIDEOS_TO_SUMMARIZE` variable within the notebook to control the workload.
    - `08_modern_llm_phi3_mini.ipynb`: Attempts to fine-tune a more modern and efficient Small Language Model (SLM), Microsoft's Phi-3 Mini (specifically `microsoft/Phi-3-mini-4k-instruct`), on the video summaries generated by notebook 07.
        - It first tries full fine-tuning, which is resource-intensive.
        - It includes a placeholder and discussion for using Parameter-Efficient Fine-Tuning (PEFT) with LoRA as an alternative if full fine-tuning is too demanding for the user's hardware (especially on MPS with limited memory).
        - Uses the `accelerate` library for easier device management.
        - **This notebook is highly experimental. Full fine-tuning of Phi-3 Mini may strain M1 Pro/Max resources. PEFT/LoRA is a recommended alternative for memory-constrained environments.**

## Setup and Dependencies

To run these notebooks, you'll need Python 3 and the following libraries. You can install them using pip:

```bash
pip install pandas beautifulsoup4 matplotlib seaborn nltk scikit-learn transformers torch youtube-transcript-api accelerate einops
# For LoRA/PEFT in notebook 08, also install:
# pip install -q peft
```

You will also need to download NLTK resources. Run this in a Python interpreter within your environment after installing nltk:

```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```
For Apple Silicon (M1/M2/M3) users wanting to use MPS for GPU acceleration, ensure you install the correct PyTorch version as detailed in the "Important Note for Apple Silicon Users" section below.

## How to Run

1.  Place your `watch-history.html` file (obtained from your Google Takeout) into the `youtube_analysis/data/` directory.
2.  Run the notebooks sequentially:
    *   Start with `01_data_cleaning.ipynb` to process your history.
    *   Then, run `02_semantic_analysis.ipynb` for EDA and topic modeling.
    *   Next, explore `03_llm_training.ipynb` for the LLM data preparation steps.
    *   Optionally, run `04_lightweight_llm_training.ipynb` to experiment with fine-tuning DistilGPT2 and generating text. Be aware that training even this lightweight model can take some time on a CPU.
    *   For users with Apple Silicon (M1/M2/M3) Macs, `05_advanced_llm_training_m1.ipynb` provides an example of fine-tuning GPT-2 using MPS. Ensure your PyTorch environment is correctly set up for MPS. Training this model will take significantly longer and use more resources than notebook 04.
    *   Notebook `06_transcript_llm_training.ipynb` offers an experimental approach to fine-tune GPT-2 on full video transcripts. **Use with caution due to high resource demand and long processing times.** Before running, ensure `youtube-transcript-api` is installed (as listed in dependencies). Critically, modify the `MAX_VIDEOS_TO_PROCESS` variable inside the notebook to a very small number (e.g., 5-10) for initial runs. Expect this notebook to take a very long time, even for a small number of videos, especially the transcript fetching and LLM training stages.
    *   Notebook `07_knowledge_extraction_llm.ipynb` introduces an advanced, experimental technique: summarizing transcripts and then fine-tuning GPT-2 on those summaries.
        - This notebook depends on `../data/video_transcripts.jsonl` being generated by notebook 06.
        - **It is extremely resource and time-intensive.** Ensure you modify `MAX_VIDEOS_TO_SUMMARIZE` inside the notebook to a very small number (e.g., 3-5) for initial runs.
        - Both the summarization of transcripts and the subsequent fine-tuning of GPT-2 will take considerable time.
    *   Notebook `08_modern_llm_phi3_mini.ipynb` is an advanced experiment in fine-tuning a modern Small Language Model (Phi-3 Mini) on video summaries.
        - This notebook depends on `../data/video_summaries.jsonl` (from notebook 07).
        - Ensure `accelerate` and `einops` are installed. For the PEFT/LoRA path, `peft` is also needed (see Setup section).
        - **This is highly experimental and resource-intensive.** Full fine-tuning of Phi-3 Mini (approx. 3.8B parameters) is attempted first. This may require significant memory and time, even on MPS. If it fails, the notebook contains guidance and placeholder code for trying PEFT/LoRA.
        - Start with a very small number of summaries if you modified notebook 07 to produce more than a few.

## Notes

- The LLM training part (`03_llm_training.ipynb`) currently only sets up data preparation and provides a placeholder for training. Actual LLM fine-tuning is computationally intensive and may require a GPU and further environment configuration.
- The LLM fine-tuning in `04_lightweight_llm_training.ipynb` uses a small model (DistilGPT2) and a very limited number of training epochs on the CPU. The primary purpose is to demonstrate the process. Generated text quality will be basic and is intended as an illustration rather than a production-ready model. For more advanced results, larger models, more data, and GPU resources would be necessary.
- The quality of semantic analysis and topic modeling can be further improved by more advanced text preprocessing, hyperparameter tuning for LDA, and exploring different embedding techniques.
- **Regarding `06_transcript_llm_training.ipynb` (Transcript Fine-tuning):**
    - This notebook is the most resource-intensive. It fetches transcripts via `youtube-transcript-api` (specifically using `get_transcript()` with preferred languages), which can be slow and is dependent on transcript availability and quality for each video.
    - **Start with `MAX_VIDEOS_TO_PROCESS` set to a very low number (e.g., 5-10) within the notebook.** Attempting to process hundreds of videos will take an extremely long time and may exhaust resources or lead to API rate-limiting.
    - Fine-tuning GPT-2 on concatenated transcripts, even for a few videos, will require significant time and memory, even with MPS acceleration.
    - The quality of the fine-tuned model will heavily depend on the amount and quality of the transcript data successfully fetched and processed.
    - Note: The transcript fetching logic in `06_transcript_llm_training.ipynb` uses exception handling that targets recent versions of the `youtube-transcript-api`. If you encounter import errors related to specific exceptions from this library, or issues with transcript fetching, you may need to adjust the `try...except` blocks in that notebook to match the version you have installed or consult the API's documentation for the latest exception types.
- **Regarding `07_knowledge_extraction_llm.ipynb` (Knowledge Extraction via Summaries):**
    - This is the most computationally demanding notebook in the sequence. It involves two major AI model processing stages: summarizing transcripts with a model like DistilBART, and then fine-tuning GPT-2 on those summaries.
    - **Strictly control `MAX_VIDEOS_TO_SUMMARIZE` (e.g., to 3-5 initially) within the notebook.** Each video summarization can take minutes, and then the LLM fine-tuning adds more time.
    - Even with MPS acceleration, expect very long runtimes for this notebook.
    - The quality of the final "knowledge-tuned" LLM depends heavily on the quality of the summaries and the (very small) amount of summary data used for fine-tuning. This is primarily a demonstration of a complex pipeline.
- **Regarding `08_modern_llm_phi3_mini.ipynb` (Phi-3 Mini Fine-tuning):**
    - This notebook pushes the boundaries of what might be feasible on consumer Apple Silicon devices for LLM fine-tuning.
    - **Full fine-tuning** of Phi-3 Mini is attempted but is very memory-intensive. Success may depend on your specific M1/M2/M3 chip configuration (Pro/Max/Ultra) and available RAM. Use a very small dataset of summaries if you encounter issues.
    - **PEFT/LoRA (Parameter-Efficient Fine-Tuning):** The notebook provides commented-out examples for using LoRA. This approach significantly reduces memory requirements and is often necessary for fine-tuning larger models on consumer hardware. You may need to install the `peft` library (`pip install -q peft`) and uncomment/adapt the LoRA code sections if full fine-tuning fails.
    - Compatibility of all features of libraries like `bitsandbytes` (for 4-bit/8-bit quantization, often used with QLoRA) with MPS can vary. The provided LoRA example is a general outline.
    - Ensure `accelerate` and `einops` are installed.

### Important Note for Apple Silicon (M1/M2/M3) Users

- **PyTorch with MPS:** To run the training in `05_advanced_llm_training_m1.ipynb`, `06_transcript_llm_training.ipynb`, `07_knowledge_extraction_llm.ipynb`, and `08_modern_llm_phi3_mini.ipynb` (and to accelerate other PyTorch tasks like in `03_llm_training.ipynb` or `04_lightweight_llm_training.ipynb` if you adapt them for MPS), you **must** have a version of PyTorch installed that supports Apple's Metal Performance Shaders (MPS).
    - Install the latest stable PyTorch version following instructions from the [official PyTorch website](https://pytorch.org/get-started/locally/) (select the appropriate options for your Mac, typically "Stable", "macOS", "Python", and "Default" or "MPS").
    - Notebooks `05`, `06`, `07`, and `08` include code to automatically detect and use MPS if available (notebook 08 uses `accelerate` which handles this).
- **Resource Usage for `05_advanced_llm_training_m1.ipynb`, `06_transcript_llm_training.ipynb`, `07_knowledge_extraction_llm.ipynb`, and `08_modern_llm_phi3_mini.ipynb`:** Fine-tuning models like GPT-2 and especially Phi-3 Mini is considerably more demanding than DistilGPT2. Expect longer training times (even with MPS) and higher memory consumption. The provided settings (e.g., batch size, epochs, number of videos processed) are starting points and may need adjustment based on your specific M1/M2/M3 configuration (Pro, Max, Ultra, RAM).
- The earlier notebooks (`01` to `04`) are designed to be more generally compatible and will run on CPU if MPS is not configured or if you are not on Apple Silicon.
```
