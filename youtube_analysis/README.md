# YouTube Watch History Analysis for Portfolio Project

This project analyzes YouTube watch history data to demonstrate skills in data cleaning, semantic analysis, and preparation for Large Language Model (LLM) fine-tuning.

## Project Structure

- `data/`: Contains the raw `watch-history.html` (user-provided) and the `cleaned_watch_history.csv` generated by the first notebook.
- `notebooks/`: Contains the Jupyter notebooks for each stage of the analysis.
    - `01_data_cleaning.ipynb`: Loads the raw `watch-history.html`, parses it, cleans the data (timestamps, duplicates, etc.), and saves the result to `cleaned_watch_history.csv`.
    - `02_semantic_analysis.ipynb`: Performs exploratory data analysis (EDA) on the cleaned watch history, including identifying top channels and viewing patterns. It also conducts semantic analysis on video titles using TF-IDF and Latent Dirichlet Allocation (LDA) for topic modeling.
    - `03_llm_training.ipynb`: Prepares the video title data for potential fine-tuning of a Large Language Model. This includes tokenization using Hugging Face `transformers` and provides a commented outline for the model training process.

## Setup and Dependencies

To run these notebooks, you'll need Python 3 and the following libraries. You can install them using pip:

```bash
pip install pandas beautifulsoup4 matplotlib seaborn nltk scikit-learn transformers torch
```

You will also need to download NLTK resources. Run this in a Python interpreter within your environment after installing nltk:

```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```

## How to Run

1.  Place your `watch-history.html` file (obtained from your Google Takeout) into the `youtube_analysis/data/` directory.
2.  Run the notebooks sequentially:
    *   Start with `01_data_cleaning.ipynb` to process your history.
    *   Then, run `02_semantic_analysis.ipynb` for EDA and topic modeling.
    *   Finally, explore `03_llm_training.ipynb` for the LLM data preparation steps.

## Notes

- The LLM training part (`03_llm_training.ipynb`) currently only sets up data preparation and provides a placeholder for training. Actual LLM fine-tuning is computationally intensive and may require a GPU and further environment configuration.
- The quality of semantic analysis and topic modeling can be further improved by more advanced text preprocessing, hyperparameter tuning for LDA, and exploring different embedding techniques.

### Note for Apple Silicon (M1/M2) Users

- The current project setup, especially for the LLM notebook (`03_llm_training.ipynb`), uses a CPU-compatible version of PyTorch. Standard Python libraries mentioned should work well on Apple Silicon.
- If you intend to pursue GPU-accelerated LLM fine-tuning on your M1/M2 Mac in the future, ensure you install a version of PyTorch that supports Apple's Metal Performance Shaders (MPS). You can find installation instructions on the [official PyTorch website](https://pytorch.org/get-started/locally/). The `transformers` library can then leverage MPS for significantly faster training.
```
